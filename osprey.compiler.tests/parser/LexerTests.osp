use namespace aves;
use namespace testing.unit;
use namespace osprey.compiler.tests;
use TT = osprey.compiler.parser.TokenType; // For brevity in some long lists
use CT = osprey.compiler.parser.ContextualType; // Also for brevity

namespace osprey.compiler.parser.tests;

public class LexerTests is TestFixture
{
	public new() { new base("osprey.compiler.parser.Lexer tests"); }

	private errors = new TestErrorManager();

	override setUp()
	{
		errors.clearAll();
	}

	// Constructor tests

	public test_Constructor1()
	{
		var file = SourceFile.createAnon("");
		var lexer;
		Assert.doesNotThrow(@{ lexer = new Lexer(file); });
		Assert.areSameRef(file, lexer.sourceFile);
	}

	public test_Constructor1WithInvalidFile()
	{
		Assert.throws(typeof(ArgumentError), @= new Lexer(""));
		Assert.throws(typeof(ArgumentNullError), @= new Lexer(null));
	}

	public test_Constructor2()
	{
		var file = SourceFile.createAnon("");
		var lexer;
		Assert.doesNotThrow(@{ lexer = new Lexer(file, LexerFlags.none); });
		Assert.areSameRef(file, lexer.sourceFile);
	}

	public test_Constructor2WithInvalidFile()
	{
		Assert.throws(typeof(ArgumentError), @= new Lexer("", LexerFlags.none));
		Assert.throws(typeof(ArgumentNullError), @= new Lexer(null, LexerFlags.none));
	}

	public test_Constructor2WithInvalidFlags()
	{
		var file = SourceFile.createAnon("");
		Assert.throws(typeof(ArgumentError), @= new Lexer(file, 0));
		Assert.throws(typeof(ArgumentError), @= new Lexer(file, "none"));
	}

	public test_Constructor3()
	{
		var file = SourceFile.createAnon("");
		var errors = new TestErrorManager();
		var lexer;
		Assert.doesNotThrow(@{ lexer = new Lexer(file, LexerFlags.none, errors); });
		Assert.areSameRef(file, lexer.sourceFile);
		Assert.areSameRef(errors, lexer.errorManager);
	}

	public test_Constructor3WithNullErrorManager()
	{
		// When the ErrorManager is null, the lexer creates its own.
		var file = SourceFile.createAnon("");
		var lexer;
		Assert.doesNotThrow(@{ lexer = new Lexer(file, LexerFlags.none, null); });
		Assert.areSameRef(file, lexer.sourceFile);
		Assert.isNotNull(lexer.errorManager);
	}

	public test_Constructor3WithInvalidFile()
	{
		var errors = new TestErrorManager();
		Assert.throws(typeof(ArgumentError), @= new Lexer("", LexerFlags.none, errors));
		Assert.throws(typeof(ArgumentNullError), @= new Lexer(null, LexerFlags.none, errors));
	}

	public test_Constructor3WithInvalidFlags()
	{
		var file = SourceFile.createAnon("");
		var errors = new TestErrorManager();
		Assert.throws(typeof(ArgumentError), @= new Lexer(file, 0, errors));
		Assert.throws(typeof(ArgumentError), @= new Lexer(file, "none", errors));
	}

	public test_Constructor3WithInvalidErrorManager()
	{
		var file = SourceFile.createAnon("");
		Assert.throws(typeof(ArgumentError), @= new Lexer(file, LexerFlags.none, "Not an ErrorManager"));
	}

	// Actual lexing tests!

	// Helper method for creating a Lexer.
	private lex(source, flags = LexerFlags.none)
	{
		var file = SourceFile.createAnon(source);
		return new Lexer(file, flags, errors);
	}

	private assertTokenMatches(token, type)
	{
		Assert.areEqual(token.type, type);
		return true;
	}

	private assertTokenMatches(token, type, value)
	{
		Assert.areEqual(token.type, type);
		Assert.areEqual(token.value, value);
		return true;
	}

	private assertIsEOF(token)
	{
		assertTokenMatches(token, TokenType.eof);
		return true;
	}

	public test_LineComment1()
	{
		var source = "//comment";
		var lexer = lex(source, LexerFlags.includeComments);
		assertTokenMatches(lexer[0], TokenType.comment, source);
		assertIsEOF(lexer[1]);
	}

	public test_LineComment2()
	{
		var lexer = lex("//comment 1\t\n//comment 2  \n", LexerFlags.includeComments);
		// The comments should not contain the newlines, but should
		// contain the trailing white space.
		assertTokenMatches(lexer[0], TokenType.comment, "//comment 1\t");
		assertTokenMatches(lexer[1], TokenType.comment, "//comment 2  ");
		assertIsEOF(lexer[2]);
	}

	public test_LineDocComment1()
	{
		var lexer = lex("///doc comment", LexerFlags.includeComments);
		assertTokenMatches(lexer[0], TokenType.comment, "///doc comment");
		assertIsEOF(lexer[1]);
	}

	public test_LineDocComment2()
	{
		var source = "///doc comment line 1\n///doc comment line 2";
		var lexer = lex(source, LexerFlags.includeComments);
		// The lexer DOES include all the lines of a doc comment, verbatim.
		assertTokenMatches(lexer[0], TokenType.comment, source);
		assertIsEOF(lexer[1]);
	}

	public test_PunctuationGeneral()
	{
		var source = r".  ,  :  ;  @  ...  ?.  !";
		var lexer = lex(source);

		var tokens = [
			TT.dot, TT.comma,
			TT.colon, TT.semicolon,
			TT.at, TT.splat,
			TT.safeAccess, TT.exclam,
		];

		Assert.collectionsMatch(lexer, tokens, assertTokenMatches);
	}

	public test_PunctuationBrackets()
	{
		var source = r"{  }  [  ]  (  )  ?(  ?[";
		var lexer = lex(source);

		// Same order as the source, of course. Sorry it's a bit hard to read.
		var tokens = [
			TT.curlyOpen, TT.curlyClose,
			TT.squareOpen, TT.squareClose,
			TT.parenOpen, TT.parenClose,
			TT.parenOpenSafe, TT.squareOpenSafe,
		];

		Assert.collectionsMatch(lexer, tokens, assertTokenMatches);
	}

	public test_PunctuationOperators()
	{
		var source = r"
			~   <   <=  >  >=  ==   !=  ?  ??  ?!
			->  +   -   |  *   /    %   &  ^   ::
			<<  >>  **  #  $   <=>
		";
		var lexer = lex(source);

		// Same order as the source, of course. Sorry it's a bit hard to read.
		var tokens = [
			TT.tilde,
			TT.less, TT.lessEqual, TT.greater, TT.greaterEqual,
			TT.doubleEqual, TT.notEqual, TT.question,
			TT.nullCoalescing, TT.nullOr,
			TT.funcApplication, TT.plus, TT.minus, TT.pipe,
			TT.multiply, TT.divide, TT.modulo, TT.ampersand,
			TT.caret, TT.concatenate, TT.shiftLeft, TT.shiftRight,
			TT.power, TT.hash, TT.dollar, TT.compare,
		];

		Assert.collectionsMatch(lexer, tokens, assertTokenMatches);
	}

	public test_PunctuationAssignOperators()
	{
		var source = r"=  +=  -=  |=  *=  /=  %=  &=  ^=  ::=  <<=  >>=  **=  #=  $=";
		var lexer = lex(source);

		// Same order as the source, of course. Sorry it's a bit hard to read.
		var tokens = [
			TT.assign,
			TT.plusAssign, TT.minusAssign, TT.pipeAssign,
			TT.mulAssign, TT.divAssign, TT.modAssign, TT.ampAssign,
			TT.caretAssign, TT.concatAssign,
			TT.shiftLeftAssign, TT.shiftRightAssign,
			TT.powerAssign, TT.hashAssign, TT.dollarAssign,
		];

		Assert.collectionsMatch(lexer, tokens, assertTokenMatches);
	}

	public test_Keywords()
	{
		var source = r"
			abstract  and        async    base         break
			catch     class      const    do           else
			enum      false      finally  for          function
			global    if         in       inheritable  is
			iter      namespace  new      next         not
			null      operator   or       overridable  override
			private   protected  public   ref          refeq
			return    static     this     throw        true
			try       typeof     use      var          while
			with      xor        yield
		";
		var lexer = lex(source);

		// Same order as the source, of course. Sorry it's a bit hard to read.
		var tokens = [
			TT.\abstract, TT.\and,       TT.\async,   TT.\base,        TT.\break,
			TT.\catch,    TT.\class,     TT.\const,   TT.\do,          TT.\else,
			TT.\enum,     TT.\false,     TT.\finally, TT.\for,         TT.\function,
			TT.\global,   TT.\if,        TT.\in,      TT.\inheritable, TT.\is,
			TT.\iter,     TT.\namespace, TT.\new,     TT.\next,        TT.\not,
			TT.\null,     TT.\operator,  TT.\or,      TT.\overridable, TT.\override,
			TT.\private,  TT.\protected, TT.\public,  TT.\ref,         TT.\refeq,
			TT.\return,   TT.\static,    TT.\this,    TT.\throw,       TT.\true,
			TT.\try,      TT.\typeof,    TT.\use,     TT.\var,         TT.\while,
			TT.\with,     TT.\xor,       TT.\yield,
		];

		Assert.collectionsMatch(lexer, tokens, assertTokenMatches);
	}

	public test_KeywordEscapes()
	{
		var source = r"
			\abstract  \and        \async    \base         \break
			\catch     \class      \const    \do           \else
			\enum      \false      \finally  \for          \function
			\global    \if         \in       \inheritable  \is
			\iter      \namespace  \new      \next         \not
			\null      \operator   \or       \overridable  \override
			\private   \protected  \public   \ref          \refeq
			\return    \static     \this     \throw        \true
			\try       \typeof     \use      \var          \while
			\with      \xor        \yield
		";
		var lexer = lex(source);

		for token in lexer
		{
			Assert.areEqual(token.type, TT.identifier);
			Assert.areEqual(token.contextualType, CT.none);
		}
	}

	public test_ContextualKeywords()
	{
		var source = r"
			get  set  to  where  version
			__primitive  __init_type  __extern  __named_const  __get_argc
		";
		var lexer = lex(source);

		// Same order as the source, of course.
		var tokens = [
			CT.get, CT.set, CT.to, CT.where, CT.version,
			CT.primitive, CT.initType, CT.extern, CT.namedConst, CT.getArgc
		];

		Assert.collectionsMatch(lexer, tokens, @(token, contextualType)
		{
			Assert.areEqual(token.type, TT.identifier);
			Assert.isOfType(token, typeof(Identifier));
			Assert.areEqual(token.contextualType, contextualType);
			return true;
		});
	}

	public test_ContextualKeywordEscapes()
	{
		var source = r"
			\get  \set  \to  \where  \version
			\__primitive  \__init_type  \__extern  \__named_const  \__get_argc
		";
		var lexer = lex(source);

		for token in lexer
		{
			Assert.areEqual(token.type, TT.identifier);
			Assert.isOfType(token, typeof(Identifier));
			Assert.areEqual(token.contextualType, CT.none);
		}
	}
}
